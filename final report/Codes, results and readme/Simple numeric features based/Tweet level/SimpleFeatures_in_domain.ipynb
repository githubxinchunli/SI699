{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "genuine_accounts_users = pd.read_csv('cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/users.csv')\n",
    "genuine_accounts_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/tweets.csv')\n",
    "social_spambots_2_users = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_2.csv/users.csv')\n",
    "social_spambots_2_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_2.csv/tweets.csv')\n",
    "social_spambots_3_users = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_2.csv/users.csv')\n",
    "social_spambots_3_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_2.csv/tweets.csv')\n",
    "traditional_spambots_1_users = pd.read_csv('cresci-2017.csv/datasets_full.csv/traditional_spambots_1.csv/users.csv', low_memory=False)\n",
    "traditional_spambots_1_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/traditional_spambots_1.csv/tweets.csv', low_memory=False)\n",
    "fake_followers_users=pd.read_csv('cresci-2017.csv/datasets_full.csv/fake_followers.csv/users.csv', low_memory=False)\n",
    "fake_followers_tweets=pd.read_csv('cresci-2017.csv/datasets_full.csv/fake_followers.csv/tweets.csv', low_memory=False)\n",
    "russian_troll_users = pd.read_csv('russian-troll-tweets/users.csv', low_memory=False)\n",
    "russian_troll_tweets = pd.read_csv('russian-troll-tweets/tweets.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = genuine_accounts_users\n",
    "a2 = genuine_accounts_tweets\n",
    "b1=fake_followers_users\n",
    "b2= fake_followers_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "a1.rename(columns={'id': 'user_id'}, inplace=True)\n",
    "a_merged1 = pd.merge(a1, a2, on = 'user_id', how = 'outer').sample(n=1000, random_state=1)\n",
    "a_merged2 = pd.merge(a1, a2, on = 'user_id', how = 'outer').drop(index=a_merged1.index).sample(n=1000, random_state=0)\n",
    "b1.rename(columns={'id': 'user_id'}, inplace=True)\n",
    "b_merged1 = pd.merge(b1, b2, on = 'user_id', how = 'outer').sample(n=1000, random_state=1)\n",
    "b_merged2 = pd.merge(b1, b2, on = 'user_id', how = 'outer').drop(index=b_merged1.index).sample(n=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_merged1['label']=int(0)\n",
    "a_merged2['label']=int(0)\n",
    "b_merged1['label']=int(1)\n",
    "b_merged2['label']=int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_merged1=a_merged1[['user_id', 'statuses_count', 'followers_count', 'friends_count', 'utc_offset', \n",
    "                   'profile_use_background_image', 'favourites_count', 'listed_count','retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_merged2=a_merged2[['user_id', 'statuses_count', 'followers_count', 'friends_count', 'utc_offset', \n",
    "                   'profile_use_background_image', 'favourites_count', 'listed_count','retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_merged1=b_merged1[['user_id', 'statuses_count', 'followers_count', 'friends_count', 'utc_offset', \n",
    "                   'profile_use_background_image', 'favourites_count', 'listed_count', 'retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_merged2=b_merged2[['user_id', 'statuses_count', 'followers_count', 'friends_count', 'utc_offset', \n",
    "                   'profile_use_background_image', 'favourites_count', 'listed_count','retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_agg = pd.concat([a_merged1, b_merged1])\n",
    "a1b1_agg=pd.concat([a_merged2, b_merged2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_merged3 = pd.merge(b1, b2, on = 'user_id', how = 'outer').drop(index=b_merged1.index).drop(index=b_merged2.index).sample(n=10, random_state=0)\n",
    "b_merged3['label']=int(1)\n",
    "b_merged3=b_merged3[['user_id', 'statuses_count', 'followers_count', 'friends_count', 'utc_offset', 'profile_use_background_image', 'favourites_count', 'listed_count','retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions', 'label']]\n",
    "ab_agg = pd.concat([ab_agg, b_merged3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240000, 15)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ab_agg.columns:\n",
    "    ab_agg[column]=ab_agg[column].fillna(ab_agg[ab_agg[column].isnull()==False][column].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in a1b1_agg.columns:\n",
    "    a1b1_agg[column]=a1b1_agg[column].fillna(a1b1_agg[a1b1_agg[column].isnull()==False][column].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "ab_agg=shuffle(ab_agg)\n",
    "a1b1_agg=shuffle(a1b1_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ab_agg.iloc[:,1:14]\n",
    "y_train=ab_agg['label']\n",
    "X_test = a1b1_agg.iloc[:,1:14]\n",
    "y_test=a1b1_agg['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.83     70000\n",
      "           1       0.78      0.98      0.87     70000\n",
      "\n",
      "   micro avg       0.85      0.85      0.85    140000\n",
      "   macro avg       0.88      0.85      0.85    140000\n",
      "weighted avg       0.88      0.85      0.85    140000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a logistic regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(penalty=\"l1\",random_state=0).fit(X_train_transformed, y_train)\n",
    "import numpy as np\n",
    "predicted = lr_model.predict(X_test_transformed)\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9872071428571428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     70000\n",
      "           1       0.98      1.00      0.99     70000\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    140000\n",
      "   macro avg       0.99      0.99      0.99    140000\n",
      "weighted avg       0.99      0.99      0.99    140000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a Random Forest Classifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(max_depth=5, criterion='entropy',\n",
    "                                   random_state=0).fit(X_train_transformed, y_train)\n",
    "import numpy as np\n",
    "predicted = rf_model.predict(X_test_transformed)\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDBoosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9987357142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     70000\n",
      "           1       1.00      1.00      1.00     70000\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    140000\n",
      "   macro avg       1.00      1.00      1.00    140000\n",
      "weighted avg       1.00      1.00      1.00    140000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a Random Forest Classifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gdb_model = GradientBoostingClassifier(learning_rate=1.0, max_depth=3, random_state=0).fit(X_train_transformed, y_train)\n",
    "import numpy as np\n",
    "predicted = gdb_model.predict(X_test_transformed)\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7916642857142857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.63      0.75     70000\n",
      "           1       0.72      0.95      0.82     70000\n",
      "\n",
      "   micro avg       0.79      0.79      0.79    140000\n",
      "   macro avg       0.82      0.79      0.79    140000\n",
      "weighted avg       0.82      0.79      0.79    140000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a Random Forest Classifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(gamma='auto', max_iter=100).fit(X_train_transformed, y_train)\n",
    "import numpy as np\n",
    "predicted = svm_model.predict(X_test_transformed)\n",
    "print(np.mean(predicted == y_test))\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
