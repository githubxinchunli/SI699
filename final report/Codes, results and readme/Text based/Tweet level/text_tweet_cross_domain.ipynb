{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "genuine_accounts_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/tweets.csv', low_memory=False)\n",
    "traditional_spambots_1_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/traditional_spambots_1.csv/tweets.csv', low_memory=False)\n",
    "fake_followers_tweets=pd.read_csv('cresci-2017.csv/datasets_full.csv/fake_followers.csv/tweets.csv', low_memory=False)\n",
    "social_spambots_3_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_3.csv/tweets.csv', low_memory=False)\n",
    "social_spambots_2_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_2.csv/tweets.csv', low_memory=False)\n",
    "russian_troll_tweets = pd.read_csv('russian-troll-tweets/tweets.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = genuine_accounts_tweets\n",
    "b= social_spambots_2_tweets\n",
    "c= social_spambots_3_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['label']=int(0)\n",
    "b['label']=int(1)\n",
    "c['label']=int(1)\n",
    "\n",
    "import random\n",
    "a1 = a.sample(n=1000, random_state=1)\n",
    "a2 = a.drop(index=a1.index).sample(n=1000, random_state=0)\n",
    "b1 = b.sample(n=1000, random_state=1)\n",
    "c1 = c.sample(n=1000, random_state=1)\n",
    "b2 = c.drop(index=c1.index).sample(n=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=pd.concat([b1, a1, b2], axis=0)\n",
    "ab_text = ab[['text', 'label']]\n",
    "ab_text = ab_text.fillna('None')\n",
    "\n",
    "ac=pd.concat([c, a2], axis=0)\n",
    "ac_text=ac[['text', 'label']]\n",
    "ac_text = ac_text.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "ab_text=shuffle(ab_text)\n",
    "ac_text=shuffle(ac_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean content: \n",
    "import spacy\n",
    "#spacy.load('en')\n",
    "#from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "import os\n",
    "def cleanContent(contents):\n",
    "    contents_new=[]\n",
    "    contents_list = contents.tolist()\n",
    "    tweets_list_new=[]\n",
    "    for tweet in contents_list:\n",
    "        if 'http' in tweet:\n",
    "            index = tweet.rfind('http')\n",
    "            tweet = tweet.replace(tweet[index:], '')\n",
    "        tweets_list_new.append(tweet)\n",
    "    for content in tweets_list_new:\n",
    "        # Convert to lowercase\n",
    "        content_low=content.lower()\n",
    "        # Remove punctuation and any other non-alphabet characters\n",
    "        content_low_pnt = re.sub(r'[^\\w\\s]', '', content_low)\n",
    "        content_low_pnt = content_low_pnt.replace(os.linesep, \" \")\n",
    "        content_low_pnt = content_low_pnt.replace(\"\\t\", \" \")\n",
    "        content_low_pnt_alpha = re.sub('[^A-Za-z\\s]', '', content_low_pnt)\n",
    "        # Remove stopwords\n",
    "        words=content_low_pnt_alpha.split()\n",
    "        no_stop_words=[word for word in words if word not in STOP_WORDS]\n",
    "        content=' '.join(no_stop_words)\n",
    "        contents_new.append(content)\n",
    "    return contents_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_text.text=cleanContent(ab_text.text)\n",
    "ac_text.text=cleanContent(ac_text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_text_list=[]\n",
    "for text in ab_text.text:\n",
    "    ab_text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_text_list=[]\n",
    "for text in ac_text.text:\n",
    "    ac_text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...f',\n",
       "  max_iter=100, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC(gamma='auto', max_iter=100, random_state=0))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0805705679799968"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(ac_text_list)\n",
    "np.mean(predicted == ac_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.97      0.12    100000\n",
      "           1       0.90      0.02      0.03   1418557\n",
      "\n",
      "   micro avg       0.08      0.08      0.08   1518557\n",
      "   macro avg       0.48      0.50      0.08   1518557\n",
      "weighted avg       0.85      0.08      0.04   1518557\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  97267,    2733],\n",
       "       [1393473,   25084]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ac_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(ac_text.label, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, max_depth=20, criterion='entropy',\n",
    "                                   random_state=0))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coef = text_clf.coef_.ravel()\n",
    "#top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "#plt.figure(figsize=(15, 5))\n",
    "#colors = ['grey' if c < 0 else 'grey' for c in coef[top_positive_coefficients]]\n",
    "#plt.bar(np.arange(top_features), coef[top_positive_coefficients], color=colors)\n",
    "#feature_names = np.array(feature_names)\n",
    "#plt.xticks(np.arange(0, top_features), feature_names[top_positive_coefficients], rotation=60, ha='center')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9356250703793141"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(ac_text_list)\n",
    "np.mean(predicted == ac_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.02      0.05    100000\n",
      "           1       0.94      1.00      0.97   1418557\n",
      "\n",
      "   micro avg       0.94      0.94      0.94   1518557\n",
      "   macro avg       0.95      0.51      0.51   1518557\n",
      "weighted avg       0.94      0.94      0.91   1518557\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   2331,   97669],\n",
       "       [     88, 1418469]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ac_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(ac_text.label, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, random_state=0))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8214759143054887"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(ac_text_list)\n",
    "np.mean(predicted == ac_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.84      0.38    100000\n",
      "           1       0.99      0.82      0.90   1418557\n",
      "\n",
      "   micro avg       0.82      0.82      0.82   1518557\n",
      "   macro avg       0.62      0.83      0.64   1518557\n",
      "weighted avg       0.94      0.82      0.86   1518557\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  83709,   16291],\n",
       "       [ 254808, 1163749]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ac_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(ac_text.label, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...e, penalty='l1', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty=\"l1\",random_state=0))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9428253269386662"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(ac_text_list)\n",
    "np.mean(predicted == ac_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.84      0.66    100000\n",
      "           1       0.99      0.95      0.97   1418557\n",
      "\n",
      "   micro avg       0.94      0.94      0.94   1518557\n",
      "   macro avg       0.77      0.89      0.81   1518557\n",
      "weighted avg       0.96      0.94      0.95   1518557\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  83787,   16213],\n",
       "       [  70610, 1347947]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ac_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(ac_text.label, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
