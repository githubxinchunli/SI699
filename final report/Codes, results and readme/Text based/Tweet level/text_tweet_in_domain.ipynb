{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "genuine_accounts_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/tweets.csv', low_memory=False)\n",
    "traditional_spambots_1_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/traditional_spambots_1.csv/tweets.csv', low_memory=False)\n",
    "fake_followers_tweets=pd.read_csv('cresci-2017.csv/datasets_full.csv/fake_followers.csv/tweets.csv', low_memory=False)\n",
    "social_spambots_3_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_3.csv/tweets.csv', low_memory=False)\n",
    "social_spambots_2_tweets = pd.read_csv('cresci-2017.csv/datasets_full.csv/social_spambots_2.csv/tweets.csv', low_memory=False)\n",
    "russian_troll_tweets = pd.read_csv('russian-troll-tweets/tweets.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = genuine_accounts_tweets\n",
    "b= russian_troll_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['label']=int(0)\n",
    "b['label']=int(1)\n",
    "\n",
    "import random\n",
    "a1 = a.sample(n=1000, random_state=1)\n",
    "a2 = a.drop(index=a1.index).sample(n=1000, random_state=0)\n",
    "b1 = b.sample(n=1000, random_state=1)\n",
    "b2 = b.drop(index=b1.index).sample(n=1000, random_state=0)\n",
    "b3=b.drop(index=b1.index).drop(index=b2.index).sample(n=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "ab=pd.concat([b1, a1, b3], axis=0)\n",
    "ab_text = ab[['text', 'label']]\n",
    "ab_text = ab_text.fillna('None')\n",
    "\n",
    "a2b2=pd.concat([b2, a2], axis=0)\n",
    "a2b2_text=a2b2[['text', 'label']]\n",
    "a2b2_text = a2b2_text.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean content: \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "import os\n",
    "def cleanContent(contents):\n",
    "    contents_new=[]\n",
    "    contents_list = contents.tolist()\n",
    "    tweets_list_new=[]\n",
    "    for tweet in contents_list:\n",
    "        if 'http' in tweet:\n",
    "            index = tweet.rfind('http')\n",
    "            tweet = tweet.replace(tweet[index:], '')\n",
    "        tweets_list_new.append(tweet)\n",
    "    for content in tweets_list_new:\n",
    "        # Convert to lowercase\n",
    "        content_low=content.lower()\n",
    "        # Remove punctuation and any other non-alphabet characters\n",
    "        content_low_pnt = re.sub(r'[^\\w\\s]', '', content_low)\n",
    "        content_low_pnt = content_low_pnt.replace(os.linesep, \" \")\n",
    "        content_low_pnt = content_low_pnt.replace(\"\\t\", \" \")\n",
    "        content_low_pnt_alpha = re.sub('[^A-Za-z\\s]', '', content_low_pnt)\n",
    "        # Remove stopwords\n",
    "        words=content_low_pnt_alpha.split()\n",
    "        no_stop_words=[word for word in words if word not in STOP_WORDS]\n",
    "        content=' '.join(no_stop_words)\n",
    "        contents_new.append(content)\n",
    "    return contents_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_text.text=cleanContent(ab_text.text)\n",
    "a2b2_text.text=cleanContent(a2b2_text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_text_list=[]\n",
    "for text in ab_text.text:\n",
    "    ab_text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2b2_text_list=[]\n",
    "for text in a2b2_text.text:\n",
    "    a2b2_text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...\n",
       "  max_iter=100, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC(gamma='auto', max_iter=100))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5201125"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(a2b2_text_list)\n",
    "np.mean(predicted == a2b2_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.11      0.18     80000\n",
      "           1       0.51      0.93      0.66     80000\n",
      "\n",
      "   micro avg       0.52      0.52      0.52    160000\n",
      "   macro avg       0.56      0.52      0.42    160000\n",
      "weighted avg       0.56      0.52      0.42    160000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8524, 71476],\n",
       "       [ 5306, 74694]])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(a2b2_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(a2b2_text.label, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...mators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, max_depth=20, criterion='entropy',\n",
    "                                   random_state=0))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504375"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(a2b2_text_list)\n",
    "np.mean(predicted == a2b2_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02     80000\n",
      "           1       0.50      1.00      0.67     80000\n",
      "\n",
      "   micro avg       0.50      0.50      0.50    160000\n",
      "   macro avg       0.75      0.50      0.34    160000\n",
      "weighted avg       0.75      0.50      0.34    160000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  700, 79300],\n",
       "       [    0, 80000]])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(a2b2_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(a2b2_text.label, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, random_state=0))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84773125"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(a2b2_text_list)\n",
    "np.mean(predicted == a2b2_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84     80000\n",
      "           1       0.81      0.90      0.86     80000\n",
      "\n",
      "   micro avg       0.85      0.85      0.85    160000\n",
      "   macro avg       0.85      0.85      0.85    160000\n",
      "weighted avg       0.85      0.85      0.85    160000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[63289, 16711],\n",
       "       [ 7652, 72348]])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(a2b2_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(a2b2_text.label, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinchunli/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...e, penalty='l1', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty=\"l1\",random_state=0))])\n",
    "\n",
    "text_clf.fit(ab_text_list, ab_text.label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87690625"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(a2b2_text_list)\n",
    "np.mean(predicted == a2b2_text.label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87     80000\n",
      "           1       0.86      0.90      0.88     80000\n",
      "\n",
      "   micro avg       0.88      0.88      0.88    160000\n",
      "   macro avg       0.88      0.88      0.88    160000\n",
      "weighted avg       0.88      0.88      0.88    160000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[68226, 11774],\n",
       "       [ 7921, 72079]])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(a2b2_text.label, predicted))\n",
    "\n",
    "metrics.confusion_matrix(a2b2_text.label, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
